import time
import pandas as pd
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.keys import Keys

########################################
# 1. ì…€ë ˆë‹ˆì›€ ë“œë¼ì´ë²„ ì‹¤í–‰
########################################
def start_driver():
    options = webdriver.ChromeOptions()
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--headless")  # ë¸Œë¼ìš°ì € ì•ˆ ë³´ì´ê²Œ í•˜ë ¤ë©´ ì£¼ì„ ì œê±°
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    return driver


########################################
# 2. ëŒ“ê¸€ ëª¨ë‘ í¼ì¹˜ê¸° (ìŠ¤í¬ë¡¤ ëê¹Œì§€)
########################################
def scroll_to_bottom(driver):
    last_height = driver.execute_script("return document.body.scrollHeight")

    while True:
        driver.find_element(By.TAG_NAME, "body").send_keys(Keys.END)
        time.sleep(1.5)

        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            break
        last_height = new_height


########################################
# 3. ëŒ“ê¸€ ìˆ˜ì§‘ í•¨ìˆ˜
########################################
def get_comments_from_url(url):
    driver = start_driver()
    driver.get(url)
    time.sleep(2)

    print(f"ğŸ“Œ ê¸°ì‚¬ ì ‘ì† ì™„ë£Œ: {url}")

    # ëŒ“ê¸€ ì˜ì—­ê¹Œì§€ ìŠ¤í¬ë¡¤
    scroll_to_bottom(driver)

    # í˜ì´ì§€ HTML ê°€ì ¸ì˜¤ê¸°
    html = driver.page_source
    soup = BeautifulSoup(html, "html.parser")

    comment_blocks = soup.select(".u_cbox_text_wrap")
    print(f"ğŸ” ìˆ˜ì§‘ëœ ëŒ“ê¸€ ë¸”ë¡ ìˆ˜: {len(comment_blocks)}")

    comments = []
    for block in comment_blocks:
        try:
            comment = block.select_one(".u_cbox_contents").get_text(strip=True)
        except:
            comment = ""

        try:
            date = block.select_one(".u_cbox_date").get_text(strip=True)
        except:
            date = ""

        try:
            like = block.select_one(".u_cbox_cnt_recomm").get_text(strip=True)
        except:
            like = "0"

        try:
            dislike = block.select_one(".u_cbox_cnt_unrecomm").get_text(strip=True)
        except:
            dislike = "0"

        comments.append({
            "comment": comment,
            "date": date,
            "like": like,
            "dislike": dislike
        })

    driver.quit()
    return pd.DataFrame(comments)


########################################
# 4. ë©”ì¸ ì‹¤í–‰
########################################
if __name__ == "__main__":
    TEST_URLS = [
        "https://n.news.naver.com/mnews/article/449/0000328367",
        "https://n.news.naver.com/mnews/article/277/0005688485",
        "https://n.news.naver.com/mnews/article/214/0001465786"
    ]

    all_comments = []

    for url in TEST_URLS:
        print("\n=====================================")
        print(f"ğŸš€ ëŒ“ê¸€ ìˆ˜ì§‘ ì‹œì‘: {url}")
        print("=====================================")

        df = get_comments_from_url(url)
        print(f"â¡ ìˆ˜ì§‘ëœ ëŒ“ê¸€ ìˆ˜: {len(df)}")

        df["article_url"] = url
        all_comments.append(df)

    final_df = pd.concat(all_comments, ignore_index=True)
    final_df.to_csv("../data/raw/comments_selenium.csv", index=False, encoding="utf-8-sig")

    print("\nğŸ‰ ì™„ë£Œ! ì €ì¥ëœ íŒŒì¼:")
    print("../data/raw/comments_selenium.csv")
