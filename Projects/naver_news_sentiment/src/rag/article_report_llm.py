import os
import pandas as pd
from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

INPUT_PATH = "./data/processed/comments_with_sentiment.csv"
OUTPUT_PATH = "./data/processed/article_reports.csv"

# ============================
# 1. ë°ì´í„° ë¡œë“œ
# ============================
def load_data():
    if not os.path.exists(INPUT_PATH):
        raise FileNotFoundError(f"{INPUT_PATH} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    df = pd.read_csv(INPUT_PATH)

    # ì»¬ëŸ¼ ì´ë¦„ ì •ê·œí™”
    cols = {c.lower(): c for c in df.columns}
    comment_col = cols.get("comment", None)
    clean_col = cols.get("comment_clean", None)
    senti_col = cols.get("sentiment", None)
    url_col = cols.get("article_url", None)

    if url_col is None:
        raise ValueError("article_url ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤. (ê¸°ì‚¬ë³„ ê·¸ë£¹í•‘ì— ì‚¬ìš©)")

    if clean_col:
        df["text_for_analysis"] = df[clean_col].fillna("")
    elif comment_col:
        df["text_for_analysis"] = df[comment_col].fillna("")
    else:
        raise ValueError("comment / comment_clean ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    if senti_col is None:
        df["sentiment"] = "ì¤‘ë¦½"

    return df, url_col


# ============================
# 2. ê¸°ì‚¬ë³„ ë¦¬í¬íŠ¸ ìƒì„±
# ============================
def build_prompt(article_url, sub_df):
    total = len(sub_df)
    senti_counts = sub_df["sentiment"].value_counts().to_dict()

    # ì˜ˆì‹œ ëŒ“ê¸€ ëª‡ ê°œë§Œ ì‚¬ìš© (ë„ˆë¬´ ë§ìœ¼ë©´ í”„ë¡¬í”„íŠ¸ ê¸¸ì–´ì§)
    examples = []
    for i, row in sub_df.head(15).iterrows():
        examples.append(f"- ({row['sentiment']}) {row['text_for_analysis']}")
    examples_str = "\n".join(examples)

    prompt = f"""
ë„ˆëŠ” ë‰´ìŠ¤ ê¸°ì‚¬ì— ë‹¬ë¦° ëŒ“ê¸€ì„ ë¶„ì„í•˜ëŠ” ë°ì´í„° ë¶„ì„ê°€ì•¼.

ì•„ë˜ëŠ” íŠ¹ì • ê¸°ì‚¬ì— ëŒ€í•œ ëŒ“ê¸€ ë°ì´í„°ë‹¤.

[ê¸°ì‚¬ URL]
{article_url}

[ì „ì²´ ëŒ“ê¸€ ìˆ˜]
{total}ê°œ

[ê°ì • ë¶„í¬]
{', '.join([f"{k}: {v}ê°œ" for k, v in senti_counts.items()])}

[ëŒ“ê¸€ ì˜ˆì‹œ]
{examples_str}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, í•œêµ­ì–´ë¡œ ë‹¤ìŒ í•­ëª©ì„ í¬í•¨í•œ ê°„ë‹¨í•œ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì¤˜.

1. ì „ì²´ ëŒ“ê¸€ ë¶„ìœ„ê¸° ìš”ì•½ (2~3ë¬¸ì¥)
2. ê¸ì •ì ì¸ ë°˜ì‘ì´ ìˆë‹¤ë©´ ì–´ë–¤ ë‚´ìš©ì¸ì§€
3. ë¶€ì •ì ì¸ ë°˜ì‘ì´ ìˆë‹¤ë©´ ì–´ë–¤ ë‚´ìš©ì¸ì§€
4. ì¤‘ë¦½/ì •ë³´ ì „ë‹¬í˜• ëŒ“ê¸€ì´ ìˆë‹¤ë©´ íŠ¹ì§•
5. í•œ ë¬¸ì¥ìœ¼ë¡œ ì •ë¦¬í•œ ê²°ë¡ 

í•­ëª© ë²ˆí˜¸ë¥¼ ìœ ì§€í•´ì„œ ê¹”ë”í•˜ê²Œ bullet í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì¤˜.
"""
    return prompt


def generate_report(article_url, sub_df):
    prompt = build_prompt(article_url, sub_df)

    completion = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
    )

    return completion.choices[0].message.content


# ============================
# 3. ë©”ì¸ ì‹¤í–‰
# ============================
if __name__ == "__main__":
    df, url_col = load_data()

    reports = []

    # ê¸°ì‚¬ URLë³„ ê·¸ë£¹í•‘
    grouped = df.groupby(url_col)

    # ë„ˆë¬´ ë§ìœ¼ë©´ ìƒìœ„ Nê°œë§Œ í•  ìˆ˜ë„ ìˆìŒ
    MAX_ARTICLES = 10  # í•„ìš”í•˜ë©´ ì¡°ì •
    for idx, (article_url, sub_df) in enumerate(grouped):
        if idx >= MAX_ARTICLES:
            break

        print(f"\nğŸ“° [{idx+1}] ê¸°ì‚¬ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘: {article_url}")
        try:
            report_text = generate_report(article_url, sub_df)
        except Exception as e:
            print("âš ï¸ ì˜¤ë¥˜ ë°œìƒ:", e)
            report_text = "ìƒì„± ì‹¤íŒ¨"

        reports.append({
            "article_url": article_url,
            "comment_count": len(sub_df),
            "report": report_text
        })

    out_df = pd.DataFrame(reports)
    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)
    out_df.to_csv(OUTPUT_PATH, index=False, encoding="utf-8-sig")

    print("\nğŸ‰ ê¸°ì‚¬ë³„ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ!")
    print("ì €ì¥ ìœ„ì¹˜:", OUTPUT_PATH)
